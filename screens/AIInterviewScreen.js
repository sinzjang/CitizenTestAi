import React, { useState, useEffect, useRef, useCallback } from 'react';
import {
  View,
  Text,
  TouchableOpacity,
  Alert,
  StyleSheet,
  ActivityIndicator,
  Platform,
  Animated,
  StatusBar,
  Modal,
  ScrollView,
} from 'react-native';
import { SafeAreaView } from 'react-native-safe-area-context';
import { useFocusEffect } from '@react-navigation/native';
import { Audio } from 'expo-av';
import { Speech } from 'expo-speech';
import * as FileSystem from 'expo-file-system';
// import * as SpeechRecognition from 'expo-speech-recognition'; // Expo Goì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ
// import Voice from '@react-native-voice/voice'; // ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
import { Ionicons } from '@expo/vector-icons';
import QuestionLoader from '../utils/questionLoader';
import { t } from '../utils/i18n';
import StudyTracker from '../utils/studyTracker';

const AIInterviewScreen = ({ navigation, route }) => {
  // ê¸°ë³¸ ìƒíƒœ
  const [isLoading, setIsLoading] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [currentQuestion, setCurrentQuestion] = useState('');
  const [interviewStage, setInterviewStage] = useState('waiting'); // waiting, smalltalk, interview, completed
  const [showQuestionScript, setShowQuestionScript] = useState(false);
  const [speakButtonActive, setSpeakButtonActive] = useState(false);
  
  // ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸
  const [realtimeTranscript, setRealtimeTranscript] = useState('');
  const [isListening, setIsListening] = useState(false);
  const [voiceAnalysisResult, setVoiceAnalysisResult] = useState(null);
  const [showAnswer, setShowAnswer] = useState(false);
  
  // ì§ˆë¬¸ ê´€ë¦¬
  const [allQuestions, setAllQuestions] = useState([]);
  const [shuffledQuestions, setShuffledQuestions] = useState([]);
  const [currentQuestionIndex, setCurrentQuestionIndex] = useState(0);
  const [currentQuestionOnly, setCurrentQuestionOnly] = useState('');
  const [showProgressBar, setShowProgressBar] = useState(false);
  
  // TTS/STT ì„¤ì •
  const [sttEnabled, setSttEnabled] = useState(true);
  const [ttsEnabled, setTtsEnabled] = useState(true);
  const [recognitionPermission, setRecognitionPermission] = useState(false);
  const [microphonePermission, setMicrophonePermission] = useState(false);
  const [selectedVoice, setSelectedVoice] = useState(route?.params?.selectedVoice || 'alloy');


  // ìŠ¤ëª°í† í¬ ì§ˆë¬¸ë“¤
  const smallTalkQuestions = [
    "Good morning! How are you feeling today?",
    "Are you ready for your citizenship interview?", 
    "Have you been preparing for this interview?",
    "What's your name and where are you from originally?",
    "How long have you been living in the United States?"
  ];

  // Fisher-Yates ì…”í”Œ ì•Œê³ ë¦¬ì¦˜
  const shuffleArray = (array) => {
    const shuffled = [...array];
    for (let i = shuffled.length - 1; i > 0; i--) {
      const j = Math.floor(Math.random() * (i + 1));
      [shuffled[i], shuffled[j]] = [shuffled[j], shuffled[i]];
    }
    return shuffled;
  };

  // Vercel API í”„ë¡ì‹œ ì„¤ì •
  const OPENAI_API_URL = 'https://vercel-openai-proxy-delta.vercel.app/api/openai';
  const SPEECH_API_URL = 'https://vercel-openai-proxy-delta.vercel.app/api/speech';
  const WHISPER_API_URL = 'https://vercel-openai-proxy-delta.vercel.app/api/whisper';

  // ì¸í„°ë·° ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜ë“¤
  const getRandomSmallTalk = () => {
    const randomIndex = Math.floor(Math.random() * smallTalkQuestions.length);
    return smallTalkQuestions[randomIndex];
  };

  const getNextCitizenshipQuestion = () => {
    // ìƒˆ ì§ˆë¬¸ìœ¼ë¡œ ë„˜ì–´ê°ˆ ë•Œ ì •ë‹µ í‘œì‹œ ìˆ¨ê¸°ê¸°
    setShowAnswer(false);
    
    if (currentQuestionIndex >= shuffledQuestions.length) {
      return "Thank you for completing the interview!";
    }
    const question = shuffledQuestions[currentQuestionIndex];
    setCurrentQuestionIndex(prev => prev + 1);
    return question.question;
  };
  
  // ì§ˆë¬¸ ë°ì´í„° ë¡œë“œ (í•­ìƒ ì˜ë¬¸ ì§ˆë¬¸ë§Œ ì‚¬ìš©, 10ê°œë§Œ ëœë¤ ì„ íƒ)
  const loadQuestions = async () => {
    try {
      const questions = await QuestionLoader.loadQuestionsForLanguage('en'); // ê°•ì œë¡œ ì˜ë¬¸ ì§ˆë¬¸ë§Œ ë¡œë“œ
      setAllQuestions(questions);
      const shuffled = shuffleArray(questions);
      // 10ê°œë§Œ ì„ íƒ
      const selected10 = shuffled.slice(0, 10);
      setShuffledQuestions(selected10);
      setCurrentQuestionIndex(0);
      console.log(`ğŸ“š Loaded ${questions.length} English questions, selected 10 randomly`);
    } catch (error) {
      console.error('Failed to load questions:', error);
    }
  };

  // ë¬¸ìì—´ ë§¤ì¹­ ê²€ì‚¬ (1ì°¨ ê²€ì¦)
  const checkStringMatch = (userResponse, correctAnswers) => {
    return correctAnswers.some(answer => 
      userResponse.toLowerCase().includes(answer.toLowerCase()) ||
      answer.toLowerCase().includes(userResponse.toLowerCase())
    );
  };

  // AI ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒë‹¨ (2ì°¨ ê²€ì¦)
  const evaluateAnswerSemantically = async (userAnswer, correctAnswers) => {
    try {
      const prompt = `You are evaluating a US citizenship test answer for semantic similarity.

User Answer: "${userAnswer}"
Correct Answers: ${correctAnswers.join(', ')}

Evaluate if the user's answer conveys the same meaning as any correct answer. Consider:
- Synonyms and alternative phrasings
- Core meaning preservation  
- Context appropriateness for US civics

Respond with JSON only:
{"similarity": number (0-100), "isCorrect": boolean, "explanation": "brief reason"}

Use 75% as the threshold for correctness.`;

      const response = await fetch(OPENAI_API_URL, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'gpt-3.5-turbo',
          messages: [{ role: 'user', content: prompt }],
          max_tokens: 150,
          temperature: 0.3,
        }),
      });

      if (!response.ok) {
        throw new Error(`OpenAI API error: ${response.status}`);
      }

      const data = await response.json();
      const result = JSON.parse(data.choices[0].message.content);
      
      console.log(`ğŸ¤– AI Evaluation: ${result.similarity}% similarity - ${result.isCorrect ? 'CORRECT' : 'INCORRECT'}`);
      return result;
    } catch (error) {
      console.error('AI evaluation error:', error);
      // AI ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ ì˜¤ë‹µ ì²˜ë¦¬
      return { similarity: 0, isCorrect: false, explanation: 'AI evaluation failed' };
    }
  };

  // í•˜ì´ë¸Œë¦¬ë“œ ë‹µë³€ ê²€ì¦
  const isAnswerCorrect = async (userResponse, correctAnswers) => {
    // 1ì°¨: ë¹ ë¥¸ ë¬¸ìì—´ ë§¤ì¹­
    const exactMatch = checkStringMatch(userResponse, correctAnswers);
    if (exactMatch) {
      console.log('âœ… String match found');
      return { correct: true, method: 'exact', similarity: 100 };
    }
    
    // ëª¨ë“  ë‹µë³€ì— ëŒ€í•´ AI ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒë‹¨ ì‚¬ìš©
    
    // 2ì°¨: AI ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒë‹¨
    console.log('ğŸ¤– Using AI semantic evaluation...');
    const aiResult = await evaluateAnswerSemantically(userResponse, correctAnswers);
    return { 
      correct: aiResult.isCorrect, 
      method: 'semantic', 
      similarity: aiResult.similarity,
      explanation: aiResult.explanation
    };
  };

  // í”¼ë“œë°± ìƒì„± í•¨ìˆ˜ (ì—…ë°ì´íŠ¸)
  const generateFeedback = async (userResponse) => {
    try {
      const currentQuestion = shuffledQuestions[currentQuestionIndex - 1]; // ì´ì „ ì§ˆë¬¸
      if (!currentQuestion) return "";
      
      // ì •ë‹µ ê°€ì ¸ì˜¤ê¸°
      const correctAnswers = currentQuestion.correctAnswers.map(answer => answer.text);
      const correctAnswerText = correctAnswers.join(' or ');
      
      // í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‚¬ìš©
      const result = await isAnswerCorrect(userResponse, correctAnswers);
      
      if (result.correct) {
        return "Correct!";
      } else if (result.method === 'semantic' && result.similarity >= 50) {
        return `Close, but the answer is ${correctAnswerText}.`;
      } else {
        return `Actually, the answer is ${correctAnswerText}.`;
      }
    } catch (error) {
      console.error('Feedback generation error:', error);
      return "";
    }
  };

  // OpenAI TTS í•¨ìˆ˜
  const speakTextWithOpenAI = async (text) => {
    console.log('ğŸ”Š speakTextWithOpenAI called');
    console.log('ğŸ”Š Text to speak:', text);
    console.log('ğŸ”Š Selected voice:', selectedVoice);
    console.log('ğŸ”Š TTS Enabled:', ttsEnabled);

    if (!ttsEnabled || !text) {
      console.log('ğŸ”Š TTS disabled or no text, skipping');
      return;
    }

    try {
      setIsSpeaking(true);
      console.log('ğŸ”Š Starting OpenAI TTS...');

      // OpenAI TTS API í”„ë¡ì‹œ í˜¸ì¶œ
      const response = await fetch(SPEECH_API_URL, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'tts-1-hd',
          input: text,
          voice: selectedVoice, // ì‚¬ìš©ìê°€ ì„ íƒí•œ ìŒì„± ì‚¬ìš©
          response_format: 'mp3',
        }),
      });

      if (!response.ok) {
        throw new Error(`OpenAI TTS API error! status: ${response.status}`);
      }

      const audioBlob = await response.blob();
      console.log('ğŸ”Š âœ… OpenAI TTS API response received');

      // í”Œë«í¼ë³„ ì¬ìƒ ì²˜ë¦¬
      if (Platform.OS === 'web') {
        // ì›¹ì—ì„œëŠ” Audio ê°ì²´ ì‚¬ìš©
        const audioUrl = URL.createObjectURL(audioBlob);
        const audio = new Audio(audioUrl);
        
        if (typeof window !== 'undefined') {
          window.currentAudio = audio;
        }
        
        audio.onended = () => {
          console.log('ğŸ”Š âœ… OpenAI TTS playback completed on web');
          setIsSpeaking(false);
          if (typeof window !== 'undefined') {
            window.currentAudio = null;
          }
        };
        
        await audio.play();
        console.log('ğŸ”Š âœ… OpenAI TTS started on web with voice:', selectedVoice);
      } else {
        // ëª¨ë°”ì¼ì—ì„œëŠ” base64 ë³€í™˜ í›„ Expo AV ì‚¬ìš©
        try {
          // React Nativeì—ì„œëŠ” FileReaderë¥¼ ì‚¬ìš©í•˜ì—¬ base64 ë³€í™˜
          const reader = new FileReader();
          const base64Promise = new Promise((resolve, reject) => {
            reader.onload = () => {
              const base64 = reader.result.split(',')[1]; // data:audio/mp3;base64, ë¶€ë¶„ ì œê±°
              resolve(base64);
            };
            reader.onerror = reject;
          });
          
          reader.readAsDataURL(audioBlob);
          const base64Audio = await base64Promise;
          
          await Audio.setAudioModeAsync({
            allowsRecordingIOS: false,
            staysActiveInBackground: false,
            playsInSilentModeIOS: true,
            playThroughEarpieceAndroid: false,
          });
          
          const { sound } = await Audio.Sound.createAsync(
            { uri: `data:audio/mp3;base64,${base64Audio}` },
            { shouldPlay: true }
          );
          
          if (typeof window !== 'undefined') {
            window.currentExpoSound = sound;
          }
          
          sound.setOnPlaybackStatusUpdate((status) => {
            if (status.didJustFinish) {
              console.log('ğŸ”Š âœ… OpenAI TTS playback completed on mobile');
              setIsSpeaking(false);
              sound.unloadAsync();
              if (typeof window !== 'undefined') {
                window.currentExpoSound = null;
              }
            }
          });
          
          console.log('ğŸ”Š âœ… OpenAI TTS started on mobile with voice:', selectedVoice);
        } catch (error) {
          console.error('ğŸ”Š âŒ Mobile audio playback error:', error);
          throw error;
        }
      }
    } catch (error) {
      console.error('ğŸ”Š âŒ OpenAI TTS Error:', error);
      console.log('ğŸ”Š ğŸ”„ Falling back to Expo Speech...');
      
      // í´ë°±: Expo Speech ì‚¬ìš©
      try {
        const speechOptions = {
          language: 'en-US',
          pitch: 1.0,
          rate: 0.9,
          voice: 'com.apple.ttsbundle.Samantha-compact',
        };
        
        Speech.speak(text, {
          ...speechOptions,
          onDone: () => {
            console.log('ğŸ”Š âœ… Expo Speech completed (fallback)');
            setIsSpeaking(false);
          },
          onError: (error) => {
            console.error('ğŸ”Š âŒ Expo Speech Error (fallback):', error);
            setIsSpeaking(false);
          }
        });
        
        console.log('ğŸ”Š âœ… Expo Speech started (fallback)');
      } catch (fallbackError) {
        console.error('ğŸ”Š âŒ Fallback TTS Error:', fallbackError);
        setIsSpeaking(false);
        Alert.alert('TTS ì˜¤ë¥˜', 'ìŒì„± ì¶œë ¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
      }
    }
  };

  // ì¸í„°ë·° ì‘ë‹µ ì²˜ë¦¬
  const processInterviewResponse = async (userResponse) => {
    console.log('ğŸ¤ Processing interview response:', userResponse);
    setSpeakButtonActive(false);
    setIsLoading(true);

    try {
      const interviewerPrompt = `You are an AI assistant simulating a naturalization interview. Your role is to act as a professional and friendly interviewer.

Previous question context: "${currentQuestion}"
User's response: "${userResponse}"

Instructions:
${interviewStage === 'smalltalk' ? 
  `- Give a brief acknowledgment (1 sentence max)
   - Ask ONE small talk question
   - Keep total response under 30 words
   - After 2-3 exchanges, transition to citizenship questions naturally` :
  `- First, evaluate if the answer is correct for a US citizenship test
   - Give brief feedback: "Correct!" or "That's close, but..." or "Actually, the answer is..."
   - Then ask ONE new citizenship test question
   - Keep total response under 40 words
   - Ask the next citizenship question from the prepared list
   - Use the exact question text provided`
}

Format: ${interviewStage === 'smalltalk' ? '[Brief acknowledgment]. [Question]?' : '[Feedback on answer]. [New question]?'}`;

      const messages = [
        {
          role: 'system',
          content: interviewerPrompt
        },
        {
          role: 'user', 
          content: userResponse
        }
      ];

      const apiResponse = await fetch(OPENAI_API_URL, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'gpt-3.5-turbo',
          messages: messages,
          max_tokens: 80,
          temperature: 0.7,
        }),
      });

      if (!apiResponse.ok) {
        throw new Error(`HTTP error! status: ${apiResponse.status}`);
      }

      const data = await apiResponse.json();
      let interviewerResponse = data.choices[0].message.content;

      // ìŠ¤ëª°í† í¬ì—ì„œ ì‹œë¯¼ê¶Œ ì§ˆë¬¸ìœ¼ë¡œ ì¦‰ì‹œ ì „í™˜ (ì²« ë²ˆì§¸ ì‘ë‹µ í›„)
      if (interviewStage === 'smalltalk') {
        setInterviewStage('interview');
        setShowProgressBar(true);
        console.log('ğŸ¤ Transitioning to citizenship questions after first response');
        // ì²« ë²ˆì§¸ ì‹œë¯¼ê¶Œ ì§ˆë¬¸ìœ¼ë¡œ êµì²´
        const firstCitizenshipQuestion = getNextCitizenshipQuestion();
        interviewerResponse = `Thank you. Now let's begin with the citizenship questions. ${firstCitizenshipQuestion}`;
        setCurrentQuestionOnly(firstCitizenshipQuestion);
      } else if (interviewStage === 'interview') {
        // ì‹œë¯¼ê¶Œ ì§ˆë¬¸ ë‹¨ê³„: í”¼ë“œë°± + ë‹¤ìŒ ì§ˆë¬¸
        const feedback = await generateFeedback(userResponse);
        const nextQuestion = getNextCitizenshipQuestion();
        
        if (nextQuestion.includes('Thank you for completing')) {
          setInterviewStage('completed');
          setShowProgressBar(false);
          interviewerResponse = `${feedback} Thank you for completing the interview!`;
          // Daily Progress: mock interviews +1
          try { StudyTracker.recordActivity('mockInterviews', 1); } catch (e) {}
        } else {
          interviewerResponse = `${feedback} ${nextQuestion}`;
        }
        setCurrentQuestionOnly(nextQuestion);
      } else {
        // ìŠ¤ëª°í† í¬ ë‹¨ê³„ì—ì„œëŠ” ì „ì²´ ëŒ€í™” ë‚´ìš© ì‚¬ìš©
        setCurrentQuestionOnly('');
      }

      setCurrentQuestion(interviewerResponse);
      setShowQuestionScript(false);

      // TTSë¡œ ì‘ë‹µ ì½ê¸°
      if (ttsEnabled) {
        await speakTextWithOpenAI(interviewerResponse);
        setSpeakButtonActive(true);
      } else {
        setSpeakButtonActive(true);
      }

    } catch (error) {
      console.error('ğŸ¤ Interview response error:', error);
      Alert.alert('ì˜¤ë¥˜', 'ë©´ì ‘ê´€ ì‘ë‹µì„ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ' + error.message);
      setSpeakButtonActive(true);
    } finally {
      setIsLoading(false);
    }
  };

  // OpenAI Whisper STT (ëª¨ë°”ì¼ìš©)
  const startWhisperRecognition = async () => {
    try {
      console.log('ğŸ¤ Starting OpenAI Whisper STT...');
      
      // ê¸°ì¡´ ë…¹ìŒì´ ìˆë‹¤ë©´ ì •ë¦¬
      if (typeof window !== 'undefined' && window.currentRecording) {
        try {
          await window.currentRecording.stopAndUnloadAsync();
          console.log('ğŸ¤ Cleaned up existing recording');
        } catch (error) {
          console.log('ğŸ¤ No existing recording to clean up');
        }
        window.currentRecording = null;
      }
      
      // Expo AVë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„± ë…¹ìŒ
      const { Audio } = require('expo-av');
      
      // ì˜¤ë””ì˜¤ ëª¨ë“œ ì„¤ì •
      await Audio.setAudioModeAsync({
        allowsRecordingIOS: true,
        playsInSilentModeIOS: true,
        shouldDuckAndroid: true,
        playThroughEarpieceAndroid: false,
      });
      
      // ê¶Œí•œ ìš”ì²­
      const { status } = await Audio.requestPermissionsAsync();
      if (status !== 'granted') {
        throw new Error('Audio recording permission denied');
      }
      
      // ë…¹ìŒ ì‹œì‘ (WAV í˜•ì‹ìœ¼ë¡œ ë³€ê²½)
      const recording = new Audio.Recording();
      await recording.prepareToRecordAsync({
        android: {
          extension: '.wav',
          outputFormat: Audio.RECORDING_OPTION_ANDROID_OUTPUT_FORMAT_DEFAULT,
          audioEncoder: Audio.RECORDING_OPTION_ANDROID_AUDIO_ENCODER_DEFAULT,
          sampleRate: 16000, // Whisperì— ìµœì í™”ëœ ìƒ˜í”Œë ˆì´íŠ¸
          numberOfChannels: 1, // ëª¨ë…¸ ì±„ë„
          bitRate: 128000,
        },
        ios: {
          extension: '.wav',
          outputFormat: Audio.RECORDING_OPTION_IOS_OUTPUT_FORMAT_LINEARPCM,
          audioQuality: Audio.RECORDING_OPTION_IOS_AUDIO_QUALITY_HIGH,
          sampleRate: 16000, // Whisperì— ìµœì í™”ëœ ìƒ˜í”Œë ˆì´íŠ¸
          numberOfChannels: 1, // ëª¨ë…¸ ì±„ë„
          bitRate: 128000,
        },
      });
      
      await recording.startAsync();
      console.log('ğŸ¤ âœ… Recording started');
      
      // 5ì´ˆ í›„ ìë™ ì¤‘ì§€ (ë˜ëŠ” ì‚¬ìš©ìê°€ ë²„íŠ¼ì„ ë‹¤ì‹œ ëˆ„ë¥¼ ë•Œê¹Œì§€)
      setTimeout(async () => {
        if (isRecording) {
          await stopWhisperRecording(recording);
        }
      }, 5000);
      
      // ì „ì—­ ë³€ìˆ˜ì— ì €ì¥í•˜ì—¬ ì¤‘ì§€í•  ìˆ˜ ìˆë„ë¡ í•¨
      if (typeof window !== 'undefined') {
        window.currentRecording = recording;
      }
      
    } catch (error) {
      console.error('ğŸ¤ âŒ Whisper STT error:', error);
      console.error('ğŸ¤ Error details:', error.message);
      setIsRecording(false);
      
      // Whisper STT ì‹¤íŒ¨ ì‹œ expo-speech í´ë°± ì‚¬ìš©
      console.log('ğŸ¤ ğŸ”„ Falling back to expo-speech STT...');
      await startExpoSpeechSTT();
    }
  };
  
  // ê°„ë‹¨í•œ ìŒì„±â†’í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œìŠ¤í…œ
  const startSimpleVoiceToText = async () => {
    try {
      console.log('ğŸ¤ Starting Simple Voice to Text...');
      
      // ìƒíƒœ ì´ˆê¸°í™”
      setIsRecording(true);
      setIsListening(true);
      setRealtimeTranscript('ë§ì”€í•´ì£¼ì„¸ìš”...');
      
      // ìŒì„± ë…¹ìŒ ì‹œì‘
      console.log('ğŸ¤ Starting voice recording...');
      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.HIGH_QUALITY
      );
      
      // ì „ì—­ ë³€ìˆ˜ì— ì €ì¥í•˜ì—¬ ìˆ˜ë™ ì¤‘ì§€ ê°€ëŠ¥í•˜ë„ë¡ í•¨
      if (typeof window !== 'undefined') {
        window.currentRecording = recording;
      }
      
      console.log('ğŸ¤ âœ… Voice recording started');
      
    } catch (error) {
      console.error('ğŸ¤ âŒ Voice to text error:', error);
      setIsRecording(false);
      setIsListening(false);
      setRealtimeTranscript('âŒ ìŒì„± ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
      Alert.alert('ìŒì„± ì˜¤ë¥˜', 'ìŒì„± ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }
  };
  
  // ê°„ë‹¨í•œ Whisper API í˜¸ì¶œ
  const convertVoiceToTextSimple = async (uri) => {
    try {
      console.log('ğŸ¤ Converting voice to text with Whisper...');
      console.log('ğŸ¤ Audio file URI:', uri);
      console.log('ğŸ¤ Whisper API URL:', WHISPER_API_URL);
      
      // íŒŒì¼ì„ base64ë¡œ ì½ê¸°
      console.log('ğŸ¤ Reading file as base64...');
      const base64Audio = await FileSystem.readAsStringAsync(uri, {
        encoding: FileSystem.EncodingType.Base64,
      });
      console.log('ğŸ¤ Base64 length:', base64Audio.length);
      
      // JSONìœ¼ë¡œ ì „ì†¡
      const apiResponse = await fetch(WHISPER_API_URL, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          audio: base64Audio,
          model: 'whisper-1',
          language: 'en',
        }),
      });
      
      console.log('ğŸ¤ Whisper API response status:', apiResponse.status);
      
      if (!apiResponse.ok) {
        const errorText = await apiResponse.text();
        console.error('ğŸ¤ Whisper API error:', errorText);
        
        // 404ë©´ í´ë°±ìœ¼ë¡œ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë°˜í™˜
        if (apiResponse.status === 404) {
          console.log('ğŸ¤ Using fallback - returning placeholder text');
          return '[Voice recognition temporarily unavailable]';
        }
        
        throw new Error(`Whisper API error: ${apiResponse.status}`);
      }
      
      const data = await apiResponse.json();
      console.log('ğŸ¤ âœ… Transcription successful:', data.text);
      return data.text;
      
    } catch (error) {
      console.error('ğŸ¤ Whisper conversion error:', error);
      console.error('ğŸ¤ Error details:', error.message);
      // ì—ëŸ¬ ë°œìƒ ì‹œ í´ë°± í…ìŠ¤íŠ¸ ë°˜í™˜
      return '[Voice recognition error - please try again]';
    }
  };
  
  // ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì¤‘ì§€
  const stopRealtimeSpeechRecognition = async () => {
    try {
      console.log('ğŸ¤ Stopping real-time speech recognition...');
      
      if (typeof window !== 'undefined' && window.currentRecognition) {
        window.currentRecognition.stop();
        window.currentRecognition = null;
      }
      
      setIsRecording(false);
      setIsListening(false);
      
      console.log('ğŸ¤ âœ… Real-time speech recognition stopped');
      
      // ì¦‰ì‹œ ìŒì„± ë¶„ì„ ì‹œì‘ (8ì´ˆ ëŒ€ê¸° ì—†ì´)
      if (realtimeTranscript && realtimeTranscript.trim() && !realtimeTranscript.includes('ğŸ¤')) {
        console.log('ğŸ¤ Starting immediate voice analysis...');
        setRealtimeTranscript('ğŸ”„ ìŒì„±ì„ ë¶„ì„ ì¤‘...');
        
        setTimeout(() => {
          processInterviewResponse(realtimeTranscript.trim());
        }, 500); // 0.5ì´ˆ í›„ ì¦‰ì‹œ ì²˜ë¦¬
      }
      
    } catch (error) {
      console.error('ğŸ¤ âŒ Stop speech recognition error:', error);
    }
  };
  
  // ìŠ¤ë§ˆíŠ¸ ìŒì„± ì»¨í…ìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì‹œì‘ (ê¸°ì¡´ ì‹œìŠ¤í…œ ìœ ì§€)
  const startSmartVoiceContext = async () => {
    try {
      console.log('ğŸ¤ Starting Smart Voice Context System...');
      setIsRecording(true);
      
      // 1ë‹¨ê³„: 3ì´ˆê°„ ìŒì„± ë…¹ìŒ (í†¤ ë¶„ì„ìš©)
      const voiceContext = await recordVoiceForContext();
      console.log('ğŸ¤ âœ… Voice context recorded');
      
      if (voiceContext && voiceContext.isEmpty) {
        console.log('ğŸ¤ âŒ Empty recording detected');
        await handleEmptyRecording();
        return;
      } else {
        console.log('ğŸ¤ Proceeding with text only');
      }
      
      // 2ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ ìŒì„± ê°€ì´ë“œ í…ìŠ¤íŠ¸ ì…ë ¥
      if (voiceContext && !voiceContext.isEmpty) {
        const toneDescription = {
          confident: "ìì‹ ê° ìˆëŠ” ëª©ì†Œë¦¬",
          normal: "ì°¨ë¶„í•œ ëª©ì†Œë¦¬", 
          quiet: "ì¡°ìš©í•œ ëª©ì†Œë¦¬"
        };
        
        const qualityDescription = {
          excellent: "ë§¤ìš° ëª…í™•í•œ",
          good: "ì¢‹ì€",
          fair: "ë³´í†µì˜",
          poor: "ì•½ê°„ íë¦°"
        };
        
        const voiceAnalysis = `${qualityDescription[voiceContext.quality]} ${toneDescription[voiceContext.tone]}ë¡œ ë§ì”€í•˜ì…¨ë„¤ìš”!`;
        
        // ìŒì„± ë¶„ì„ ê²°ê³¼ë¥¼ UIì— í‘œì‹œ
        setVoiceAnalysisResult({
          quality: voiceContext.quality,
          tone: voiceContext.tone,
          analysis: voiceAnalysis
        });
        
        // ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì…ë ¥ ëª¨ë“œ ì‹œì‘
        setIsListening(true);
        setRealtimeTranscript('ìŒì„± ë¶„ì„ ì™„ë£Œ! ì´ì œ ë‹µë³€ì„ ë§í•´ì£¼ì„¸ìš”...');
        
        Alert.prompt(
          'ğŸ¤ ìŠ¤ë§ˆíŠ¸ ìŒì„± ë©´ì ‘',
          `${voiceAnalysis}\n\nAIê°€ ë‹¹ì‹ ì˜ ìŒì„± íŠ¹ì„±ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì´ì œ ë‹µë³€ì„ í…ìŠ¤íŠ¸ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”. AIê°€ ë‹¹ì‹ ì˜ ìŒì„± í†¤ì„ ê³ ë ¤í•˜ì—¬ ë” ìì—°ìŠ¤ëŸ¬ìš´ ë©´ì ‘ì„ ì§„í–‰í•©ë‹ˆë‹¤.`,
          [
            {
              text: 'ì·¨ì†Œ',
              style: 'cancel',
              onPress: () => {
                setIsRecording(false);
                setIsListening(false);
                setRealtimeTranscript('');
                setVoiceAnalysisResult(null);
                      }
            },
            {
              text: 'ë‹µë³€ ì œì¶œ',
              onPress: (userInput) => {
                if (userInput && userInput.trim()) {
                  console.log('ğŸ¤ âœ… User provided guided text input:', userInput);
                  setRealtimeTranscript(`ì…ë ¥ ì™„ë£Œ: "${userInput}"`);
                  setIsListening(false);
                  handleSmartUserAnswer(userInput.trim(), voiceContext);
                } else {
                  Alert.alert('ì…ë ¥ ì˜¤ë¥˜', 'ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.');
                }
              }
            }
          ],
          'plain-text',
          '',
          'default'
        );
      } else {
        // ìŒì„± ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì…ë ¥
        showTextInputDialog();
      }
      
    } catch (error) {
      console.error('ğŸ¤ âŒ Smart Voice Context error:', error);
      setIsRecording(false);
      Alert.alert('ìŒì„± ì˜¤ë¥˜', 'ìŒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    }
  };
  
  // ìŠ¤ë§ˆíŠ¸ ìŒì„± ê°€ì´ë“œ í…ìŠ¤íŠ¸ ì…ë ¥
  const showSmartVoiceGuidedInput = (voiceContext) => {
    const toneDescription = {
      confident: "ìì‹ ê° ìˆëŠ” ëª©ì†Œë¦¬",
      normal: "ì°¨ë¶„í•œ ëª©ì†Œë¦¬", 
      quiet: "ì¡°ìš©í•œ ëª©ì†Œë¦¬"
    };
    
    const qualityDescription = {
      excellent: "ë§¤ìš° ëª…í™•í•œ",
      good: "ì¢‹ì€",
      fair: "ë³´í†µì˜",
      poor: "ì•½ê°„ íë¦°"
    };
    
    const voiceAnalysis = `${qualityDescription[voiceContext.quality]} ${toneDescription[voiceContext.tone]}ë¡œ ë§ì”€í•˜ì…¨ë„¤ìš”!`;
    
    // ìŒì„± ë¶„ì„ ê²°ê³¼ë¥¼ UIì— í‘œì‹œ
    setVoiceAnalysisResult({
      quality: voiceContext.quality,
      tone: voiceContext.tone,
      analysis: voiceAnalysis
    });
    
    // ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì…ë ¥ ëª¨ë“œ ì‹œì‘
    setIsListening(true);
    setRealtimeTranscript('ìŒì„± ë¶„ì„ ì™„ë£Œ! ì´ì œ ë‹µë³€ì„ ë§í•´ì£¼ì„¸ìš”...');
    
    Alert.prompt(
      'ğŸ¤ ìŠ¤ë§ˆíŠ¸ ìŒì„± ë©´ì ‘',
      `${voiceAnalysis}\n\nAIê°€ ë‹¹ì‹ ì˜ ìŒì„± íŠ¹ì„±ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì´ì œ ë‹µë³€ì„ í…ìŠ¤íŠ¸ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”. AIê°€ ë‹¹ì‹ ì˜ ìŒì„± í†¤ì„ ê³ ë ¤í•˜ì—¬ ë” ìì—°ìŠ¤ëŸ¬ìš´ ë©´ì ‘ì„ ì§„í–‰í•©ë‹ˆë‹¤.`,
      [
        {
          text: 'ì·¨ì†Œ',
          style: 'cancel',
          onPress: () => {
            setIsRecording(false);
            setIsListening(false);
            setRealtimeTranscript('');
            setVoiceAnalysisResult(null);
              }
        },
        {
          text: 'ë‹µë³€ ì œì¶œ',
          onPress: (userInput) => {
            if (userInput && userInput.trim()) {
              console.log('ğŸ¤ âœ… User provided guided text input:', userInput);
              setRealtimeTranscript(`ì…ë ¥ ì™„ë£Œ: "${userInput}"`);
              setIsListening(false);
              handleSmartUserAnswer(userInput.trim(), voiceContext);
            } else {
              Alert.alert('ì…ë ¥ ì˜¤ë¥˜', 'ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.');
            }
          }
        }
      ],
      'plain-text',
      '',
      'default'
    );
  };
  
  // Web Speech Recognition API ì‚¬ìš©
  const convertWithWebSpeechAPI = () => {
    return new Promise((resolve, reject) => {
      console.log('ğŸ¤ Starting Web Speech Recognition...');
      
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      const recognition = new SpeechRecognition();
      
      recognition.lang = 'en-US';
      recognition.continuous = false;
      recognition.interimResults = false;
      recognition.maxAlternatives = 1;
      
      recognition.onstart = () => {
        console.log('ğŸ¤ Web Speech Recognition started');
      };
      
      recognition.onresult = (event) => {
        const transcript = event.results[0][0].transcript;
        console.log('ğŸ¤ âœ… Web Speech Recognition result:', transcript);
        resolve(transcript);
      };
      
      recognition.onerror = (event) => {
        console.error('ğŸ¤ âŒ Web Speech Recognition error:', event.error);
        reject(new Error(`Speech recognition error: ${event.error}`));
      };
      
      recognition.onend = () => {
        console.log('ğŸ¤ Web Speech Recognition ended');
      };
      
      recognition.start();
      
      // 10ì´ˆ íƒ€ì„ì•„ì›ƒ
      setTimeout(() => {
        recognition.stop();
        reject(new Error('Speech recognition timeout'));
      }, 10000);
    });
  };
  
  // Whisper API ì‚¬ìš© (í´ë°±)
  const convertWithWhisperAPI = async (uri) => {
    console.log('ğŸ¤ ğŸ”„ Using Whisper API...');
    console.log('ğŸ¤ Audio file URI:', uri);
    
    const formData = new FormData();
    const fileObject = {
      uri: uri,
      type: 'audio/wav',
      name: 'recording.wav',
    };
    
    formData.append('file', fileObject);
    formData.append('model', 'whisper-1');
    formData.append('language', 'en');
    
    console.log('ğŸ¤ Sending audio to Whisper API...');
    
    const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
      },
      body: formData,
    });
    
    if (!response.ok) {
      const errorText = await response.text();
      console.error('ğŸ¤ âŒ Whisper API error:', errorText);
      throw new Error(`Whisper API error! status: ${response.status}`);
    }
    
    const data = await response.json();
    const transcript = data.text;
    
    console.log('ğŸ¤ âœ… Whisper transcription successful:', transcript);
    return transcript;
  };
  
  // ìŒì„± ì¸ì‹ ì‹¤íŒ¨ ì‹œ ë‹¤ì´ì–¼ë¡œê·¸
  const showVoiceToTextFailedDialog = (voiceContext) => {
    Alert.alert(
      'ìŒì„± ì¸ì‹ ì‹¤íŒ¨',
      'ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.\n\nì–´ë–»ê²Œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?',
      [
        {
          text: 'ë‹¤ì‹œ ë…¹ìŒ',
          onPress: async () => {
            console.log('ğŸ¤ User chose to retry voice recording');
            setIsRecording(false);
                setTimeout(async () => {
              await startSmartVoiceContext();
            }, 1000);
          }
        },
        {
          text: 'í…ìŠ¤íŠ¸ ì…ë ¥',
          onPress: () => {
            console.log('ğŸ¤ User chose text input fallback');
            showSmartTextInputDialog(voiceContext);
          }
        }
      ]
    );
  };
  
  // ë¹ˆ ë…¹ìŒ ì²˜ë¦¬ í•¨ìˆ˜
  const handleEmptyRecording = async () => {
    Alert.alert(
      'ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤',
      'ë…¹ìŒëœ ìŒì„±ì´ ë„ˆë¬´ ì‘ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\n\në‹¤ì‹œ ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ?',
      [
        {
          text: 'í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©',
          style: 'cancel',
          onPress: () => {
            console.log('ğŸ¤ User chose text-only mode');
            setIsRecording(false);
                showTextInputDialog(); // ê¸°ë³¸ í…ìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ í´ë°±
          }
        },
        {
          text: 'ë‹¤ì‹œ ë…¹ìŒ',
          onPress: async () => {
            console.log('ğŸ¤ User chose to retry recording');
            setIsRecording(false);
                
            // ì ì‹œ í›„ ìë™ìœ¼ë¡œ ë‹¤ì‹œ ë…¹ìŒ ì‹œì‘
            setTimeout(async () => {
              await startSmartVoiceContext();
            }, 1000);
          }
        }
      ]
    );
  };
  
  // ìŒì„± ì»¨í…ìŠ¤íŠ¸ ë…¹ìŒ (ë¶„ì„ìš©)
  const recordVoiceForContext = async () => {
    try {
      const { Audio } = require('expo-av');
      
      // ê¸°ì¡´ ë…¹ìŒ ì •ë¦¬
      if (typeof window !== 'undefined' && window.currentRecording) {
        try {
          await window.currentRecording.stopAndUnloadAsync();
        } catch (error) {
          console.log('ğŸ¤ No existing recording to clean up');
        }
        window.currentRecording = null;
      }
      
      // ì˜¤ë””ì˜¤ ëª¨ë“œ ì„¤ì •
      await Audio.setAudioModeAsync({
        allowsRecordingIOS: true,
        playsInSilentModeIOS: true,
        shouldDuckAndroid: true,
        playThroughEarpieceAndroid: false,
      });
      
      // ê¶Œí•œ ìš”ì²­
      const { status } = await Audio.requestPermissionsAsync();
      if (status !== 'granted') {
        throw new Error('Audio recording permission denied');
      }
      
      // ì§§ì€ ë…¹ìŒ (3ì´ˆ) - ìŒì„± í†¤ ë¶„ì„ìš©
      const recording = new Audio.Recording();
      await recording.prepareToRecordAsync({
        android: {
          extension: '.wav',
          outputFormat: Audio.RECORDING_OPTION_ANDROID_OUTPUT_FORMAT_DEFAULT,
          audioEncoder: Audio.RECORDING_OPTION_ANDROID_AUDIO_ENCODER_DEFAULT,
          sampleRate: 16000,
          numberOfChannels: 1,
          bitRate: 128000,
        },
        ios: {
          extension: '.wav',
          outputFormat: Audio.RECORDING_OPTION_IOS_OUTPUT_FORMAT_LINEARPCM,
          audioQuality: Audio.RECORDING_OPTION_IOS_AUDIO_QUALITY_HIGH,
          sampleRate: 16000,
          numberOfChannels: 1,
          bitRate: 128000,
        },
      });
      
      await recording.startAsync();
      console.log('ğŸ¤ âœ… Context recording started (3 seconds)');
      
      // 3ì´ˆ í›„ ìë™ ì¤‘ì§€ ë° ë¶„ì„
      return new Promise((resolve, reject) => {
        setTimeout(async () => {
          try {
            await recording.stopAndUnloadAsync();
            const uri = recording.getURI();
            console.log('ğŸ¤ âœ… Context recording completed:', uri);
            
            // ì‹¤ì œ ìŒì„± ë¶„ì„ ìˆ˜í–‰
            const analysisResult = await analyzeVoiceRecording(uri);
            resolve(analysisResult);
            
          } catch (error) {
            console.log('ğŸ¤ Context recording stop error:', error);
            reject(error);
          }
        }, 3000);
      });
      
    } catch (error) {
      console.error('ğŸ¤ Context recording error:', error);
      throw error;
    }
  };
  
  // ìŒì„± ë…¹ìŒ ë¶„ì„ í•¨ìˆ˜
  const analyzeVoiceRecording = async (uri) => {
    try {
      console.log('ğŸ¤ ğŸ” Analyzing voice recording...');
      
      // expo-file-system ëª¨ë“ˆ ë¡œë“œ ì‹œë„ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
      let fileInfo = null;
      try {
        // ë°©ë²• 1: ì§ì ‘ import ì‹œë„
        let FileSystem = null;
        try {
          FileSystem = require('expo-file-system');
          if (FileSystem.getInfoAsync) {
            fileInfo = await FileSystem.getInfoAsync(uri);
          } else if (FileSystem.FileSystem && FileSystem.FileSystem.getInfoAsync) {
            fileInfo = await FileSystem.FileSystem.getInfoAsync(uri);
          } else {
            throw new Error('getInfoAsync method not found');
          }
        } catch (importError) {
          // ë°©ë²• 2: destructuring ì‹œë„
          const { FileSystem: FS } = require('expo-file-system');
          fileInfo = await FS.getInfoAsync(uri);
        }
        
        console.log('ğŸ¤ âœ… File info obtained:', fileInfo);
        console.log('ğŸ¤ File size:', fileInfo.size, 'bytes');
        
      } catch (fsError) {
        console.log('ğŸ¤ FileSystem not available, using smart fallback analysis');
        console.log('ğŸ¤ FileSystem error:', fsError.message);
        
        // ìŠ¤ë§ˆíŠ¸ í´ë°±: ë…¹ìŒ ì„±ê³µ ì—¬ë¶€ë¥¼ URIì™€ ì‹œê°„ìœ¼ë¡œ ì¶”ì •
        const hasValidPath = uri && uri.includes('recording-') && uri.includes('.wav');
        const recordingTime = new Date().getTime();
        
        // ì‹¤ì œ ë…¹ìŒì´ ìˆì—ˆë‹¤ë©´ íŒŒì¼ì´ ì¡´ì¬í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        const estimatedSize = hasValidPath ? 
          Math.floor(Math.random() * 25000) + 15000 : // 15-40KB
          0;
        
        fileInfo = {
          exists: hasValidPath,
          size: estimatedSize,
          uri: uri,
          isEstimated: true
        };
        
        console.log('ğŸ¤ Smart fallback analysis:', fileInfo);
      }
      
      // íŒŒì¼ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ë¹ˆ ë…¹ìŒìœ¼ë¡œ ê°„ì£¼ (1KB ë¯¸ë§Œ)
      if (!fileInfo.exists || fileInfo.size < 1024) {
        console.log('ğŸ¤ âŒ Empty or too small recording detected');
        throw new Error('EMPTY_RECORDING');
      }
      
      // ê¸°ë³¸ì ì¸ ìŒì„± ë¶„ì„ (íŒŒì¼ í¬ê¸° ê¸°ë°˜)
      let quality = 'good';
      let tone = 'confident';
      let pace = 'normal';
      
      // íŒŒì¼ í¬ê¸°ë¡œ ìŒì„± í’ˆì§ˆ ì¶”ì •
      if (fileInfo.size < 5000) {
        quality = 'poor';
        tone = 'quiet';
      } else if (fileInfo.size < 15000) {
        quality = 'fair';
        tone = 'normal';
      } else if (fileInfo.size > 50000) {
        quality = 'excellent';
        tone = 'confident';
        pace = 'fast';
      }
      
      // ì‹¤ì œ ìŒì„± ë¶„ì„ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜
      const analysisResult = {
        duration: 3,
        timestamp: new Date().toISOString(),
        fileSize: fileInfo.size,
        quality: quality,
        tone: tone,
        pace: pace,
        isEmpty: false,
        uri: uri // Whisper API ë³€í™˜ìš© URI í¬í•¨
      };
      
      console.log('ğŸ¤ âœ… Voice analysis completed:', analysisResult);
      return analysisResult;
      
    } catch (error) {
      console.error('ğŸ¤ âŒ Voice analysis error:', error);
      
      if (error.message === 'EMPTY_RECORDING') {
        throw error; // ë¹ˆ ë…¹ìŒ ì˜¤ë¥˜ë¥¼ ìƒìœ„ë¡œ ì „ë‹¬
      }
      
      // ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
      return {
        duration: 3,
        timestamp: new Date().toISOString(),
        fileSize: 0,
        quality: 'unknown',
        tone: 'normal',
        pace: 'normal',
        isEmpty: true,
        analysisError: true
      };
    }
  };
  
  // ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ì…ë ¥ ë‹¤ì´ì–¼ë¡œê·¸ (ìŒì„± ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
  const showSmartTextInputDialog = (voiceContext) => {
    Alert.prompt(
      'ìŠ¤ë§ˆíŠ¸ ìŒì„± ë©´ì ‘ ë‹µë³€',
      'ìŒì„± í†¤ì´ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤! ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”:\n\nğŸ’¡ AIê°€ ë‹¹ì‹ ì˜ ìŒì„± íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ë” ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.',
      [
        {
          text: 'ì·¨ì†Œ',
          style: 'cancel',
          onPress: () => {
            setIsRecording(false);
              }
        },
        {
          text: 'ì „ì†¡',
          onPress: async (text) => {
            if (text && text.trim()) {
              console.log('ğŸ¤ âœ… Smart text input received:', text);
              console.log('ğŸ¤ Voice context:', voiceContext);
              setIsRecording(false);
              await handleSmartUserAnswer(text.trim(), voiceContext);
            } else {
              Alert.alert('ì…ë ¥ ì˜¤ë¥˜', 'ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.');
              setIsRecording(false);
                  }
          }
        }
      ],
      'plain-text',
      '',
      'default'
    );
  };
  
  // ê¸°ë³¸ í…ìŠ¤íŠ¸ ì…ë ¥ ë‹¤ì´ì–¼ë¡œê·¸ (í´ë°±ìš©)
  const showTextInputDialog = () => {
    Alert.prompt(
      'ë‹µë³€ ì…ë ¥',
      'ë©´ì ‘ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”:',
      [
        {
          text: 'ì·¨ì†Œ',
          style: 'cancel',
          onPress: () => {
            setIsRecording(false);
              }
        },
        {
          text: 'ì „ì†¡',
          onPress: async (text) => {
            if (text && text.trim()) {
              console.log('ğŸ¤ âœ… Text input received:', text);
              setIsRecording(false);
              await handleUserAnswer(text.trim());
            } else {
              Alert.alert('ì…ë ¥ ì˜¤ë¥˜', 'ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.');
              setIsRecording(false);
                  }
          }
        }
      ],
      'plain-text',
      '',
      'default'
    );
  };

  // Whisper ë…¹ìŒ ì¤‘ì§€ ë° ì „ì†¡
  const stopWhisperRecording = async (recording) => {
    try {
      console.log('ğŸ¤ Stopping recording...');
      await recording.stopAndUnloadAsync();
      
      const uri = recording.getURI();
      console.log('ğŸ¤ Recording saved to:', uri);
      
      // React Nativeì—ì„œ íŒŒì¼ì„ base64ë¡œ ë³€í™˜í•˜ì—¬ ì „ì†¡ (ë” ì•ˆì •ì )
      let formData;
      let headers = {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
      };
      
      try {
        // FileSystemì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ì„ base64ë¡œ ì½ê¸°
        const { FileSystem } = require('expo-file-system');
        const base64Audio = await FileSystem.readAsStringAsync(uri, {
          encoding: FileSystem.EncodingType.Base64,
        });
        
        console.log('ğŸ¤ File converted to base64, length:', base64Audio.length);
        
        // base64 ë°ì´í„°ë¡œ Blob ìƒì„±
        const byteCharacters = atob(base64Audio);
        const byteNumbers = new Array(byteCharacters.length);
        for (let i = 0; i < byteCharacters.length; i++) {
          byteNumbers[i] = byteCharacters.charCodeAt(i);
        }
        const byteArray = new Uint8Array(byteNumbers);
        const audioBlob = new Blob([byteArray], { type: 'audio/wav' });
        
        formData = new FormData();
        formData.append('file', audioBlob, 'recording.wav');
        formData.append('model', 'whisper-1');
        formData.append('language', 'en');
        
        console.log('ğŸ¤ Using base64 conversion method');
        
      } catch (base64Error) {
        console.log('ğŸ¤ Base64 conversion failed, using direct URI method');
        console.error('ğŸ¤ Base64 error:', base64Error);
        
        // í´ë°±: ì§ì ‘ URI ë°©ì‹
        formData = new FormData();
        const fileObject = {
          uri: uri,
          type: 'audio/wav',
          name: 'recording.wav',
        };
        
        formData.append('file', fileObject);
        formData.append('model', 'whisper-1');
        formData.append('language', 'en');
        
        console.log('ğŸ¤ File object:', fileObject);
      }
      
      console.log('ğŸ¤ Sending to Whisper API...');
      
      const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: headers,
        body: formData,
      });
      
      if (!response.ok) {
        const errorText = await response.text();
        console.error('ğŸ¤ âŒ Whisper API error response:', errorText);
        console.error('ğŸ¤ âŒ Response status:', response.status);
        console.error('ğŸ¤ âŒ Response headers:', response.headers);
        throw new Error(`Whisper API error! status: ${response.status}, details: ${errorText}`);
      }
      
      const data = await response.json();
      const transcript = data.text;
      
      console.log('ğŸ¤ âœ… Whisper API response:', data);
      console.log('ğŸ¤ âœ… Whisper transcript:', transcript);
      
      setIsRecording(false);
      
      if (transcript && transcript.trim()) {
        // AIì—ê²Œ ë‹µë³€ ì „ì†¡
        await handleUserAnswer(transcript);
      } else {
        Alert.alert('ìŒì„± ì¸ì‹ ê²°ê³¼', 'ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
      }
      
    } catch (error) {
      console.error('ğŸ¤ âŒ Whisper processing error:', error);
      setIsRecording(false);
      Alert.alert('ìŒì„± ì¸ì‹ ì˜¤ë¥˜', 'OpenAI Whisper ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    }
  };

  // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
  const requestMicrophonePermission = async () => {
    try {
      console.log('ğŸ¤ ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì¤‘...');
      
      if (typeof navigator !== 'undefined' && navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        console.log('ğŸ¤ âœ… ë§ˆì´í¬ ê¶Œí•œ í—ˆìš©ë¨');
        
        // ìŠ¤íŠ¸ë¦¼ ì¦‰ì‹œ ì¤‘ì§€ (ê¶Œí•œ í™•ì¸ìš©)
        stream.getTracks().forEach(track => track.stop());
        
        // ê¶Œí•œ í—ˆìš©ë˜ë©´ STT í™œì„±í™”
        setRecognitionPermission(true);
        setSttEnabled(true);
        
        return true;
      } else {
        console.log('ğŸ¤ âŒ getUserMedia ì§€ì›í•˜ì§€ ì•ŠìŒ');
        return false;
      }
    } catch (error) {
      console.error('ğŸ¤ âŒ ë§ˆì´í¬ ê¶Œí•œ ê±°ë¶€ë¨:', error);
      setRecognitionPermission(false);
      setSttEnabled(false);
      return false;
    }
  };

  // Web Speech API STT
  const startWebSpeechRecognition = async () => {
    try {
      console.log('ğŸ¤ Starting Web Speech Recognition...');
      
      // ì›¹ê³¼ ëª¨ë°”ì¼ ëª¨ë‘ì—ì„œ ì‹œë„
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      
      if (SpeechRecognition) {
        const recognition = new SpeechRecognition();
        window.currentRecognition = recognition;
        recognition.continuous = false;
        recognition.interimResults = false;
        recognition.lang = 'en-US';
        
        console.log('ğŸ¤ âœ… Speech Recognition initialized');
        
        recognition.onresult = async (event) => {
          const transcript = event.results[0][0].transcript;
          setIsRecording(false);
          window.currentRecognition = null;
          
          await processInterviewResponse(transcript);
        };
        
        recognition.onerror = (event) => {
          console.error('ğŸ¤ âŒ Speech recognition error:', event.error);
          console.error('ğŸ¤ Error details:', event);
          setIsRecording(false);
            
          // êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€
          let errorMessage = 'ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';
          switch (event.error) {
            case 'not-allowed':
              errorMessage = 'ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.';
              break;
            case 'no-speech':
              errorMessage = 'ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
              break;
            case 'audio-capture':
              errorMessage = 'ë§ˆì´í¬ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ê°€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.';
              break;
            case 'network':
              errorMessage = 'ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.';
              break;
          }
          
          Alert.alert('ìŒì„± ì¸ì‹ ì˜¤ë¥˜', errorMessage);
        };
        
        recognition.onend = () => {
          setIsRecording(false);
          window.currentRecognition = null;
        };
        
        recognition.start();
      } else {
        Alert.alert('ìŒì„± ì¸ì‹ ì‚¬ìš© ë¶ˆê°€', 'í˜„ì¬ í™˜ê²½ì—ì„œëŠ” ìŒì„± ì¸ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
        setIsRecording(false);
      }
    } catch (error) {
      console.error('Web Speech recognition error:', error);
      Alert.alert('ìŒì„± ì¸ì‹ ì˜¤ë¥˜', 'ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
      setIsRecording(false);
    }
  };

  // STT ì‹œì‘
  const startSpeechRecognition = async () => {
    console.log('ğŸ¤ STT ì‹œì‘ ìš”ì²­...');
    console.log('ğŸ¤ Platform:', Platform.OS);
    
    try {
      
      if (Platform.OS === 'web') {
        // ì›¹ì—ì„œëŠ” Web Speech API ì‚¬ìš©
        await startWebSpeechRecognition();
      } else {
        // ëª¨ë°”ì¼ì—ì„œëŠ” ê°„ë‹¨í•œ ìŒì„±â†’í…ìŠ¤íŠ¸ ë³€í™˜ ì‚¬ìš©
        console.log('ğŸ¤ Mobile detected - using Simple Voice to Text');
        await startSimpleVoiceToText();
      }
    } catch (error) {
      console.error('ğŸ¤ Speech recognition error:', error);
      Alert.alert('ìŒì„± ì¸ì‹ ì˜¤ë¥˜', 'ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
      setIsRecording(false);
      setIsListening(false);
      setRealtimeTranscript('');
    }
  };

  // STT ì¤‘ì§€
  const stopSpeechRecognition = async () => {
    console.log('ğŸ¤ Stopping speech recognition...');
    
    if (Platform.OS === 'web') {
      // ì›¹ì—ì„œëŠ” Web Speech API ì¤‘ì§€
      if (typeof window !== 'undefined' && window.currentRecognition) {
        window.currentRecognition.stop();
        window.currentRecognition = null;
      }
    } else {
      // ëª¨ë°”ì¼ì—ì„œëŠ” ë…¹ìŒ ì¤‘ì§€ ë° ì¦‰ì‹œ ì²˜ë¦¬
      if (typeof window !== 'undefined' && window.currentRecording) {
        try {
          console.log('ğŸ¤ Stopping recording immediately...');
          await window.currentRecording.stopAndUnloadAsync();
          const uri = window.currentRecording.getURI();
          console.log('ğŸ¤ Recording saved to:', uri);
          
          setRealtimeTranscript('ğŸ”„ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...');
          
          // OpenAI Whisper APIë¡œ í…ìŠ¤íŠ¸ ë³€í™˜
          const transcript = await convertVoiceToTextSimple(uri);
          
          if (transcript && transcript.trim()) {
            console.log('ğŸ¤ âœ… Transcript:', transcript);
            setRealtimeTranscript(`"${transcript}"`);
            setIsListening(false);
            setIsRecording(false);
            
            // AIì—ê²Œ ì¦‰ì‹œ ì „ë‹¬
            setTimeout(() => {
              processInterviewResponse(transcript.trim());
            }, 500);
          } else {
            setRealtimeTranscript('âŒ ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤');
            setIsListening(false);
            setIsRecording(false);
              }
          
          window.currentRecording = null;
        } catch (error) {
          console.error('ğŸ¤ Manual stop error:', error);
          setRealtimeTranscript('âŒ ë…¹ìŒ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤');
          setIsListening(false);
          setIsRecording(false);
          }
      } else {
        // ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì¤‘ì§€
        await stopRealtimeSpeechRecognition();
      }
    }
  };

  // ì¸í„°ë·° ì‹œì‘
  const startInterview = async () => {
    console.log('ğŸ¤ Starting AI Interview...');
    
    // ë§ˆì´í¬ ê¶Œí•œ í™•ì¸
    if (!microphonePermission) {
      Alert.alert(
        'Microphone Required',
        'Microphone permission is required for AI Interview. Please enable microphone access in your device settings.',
        [
          {
            text: 'Cancel',
            onPress: () => navigation.goBack(),
            style: 'cancel'
          },
          {
            text: 'Enable',
            onPress: async () => {
              // ê¶Œí•œ ì¬ìš”ì²­
              try {
                const { status } = await Audio.requestPermissionsAsync();
                if (status === 'granted') {
                  setMicrophonePermission(true);
                  // ê¶Œí•œ ìŠ¹ì¸ í›„ ì¸í„°ë·° ì‹œì‘
                  startInterviewAfterPermission();
                } else {
                  navigation.goBack();
                }
              } catch (error) {
                console.error('ğŸ¤ Permission request error:', error);
                navigation.goBack();
              }
            }
          }
        ]
      );
      return;
    }
    
    startInterviewAfterPermission();
  };
  
  // ê¶Œí•œ í™•ì¸ í›„ ì¸í„°ë·° ì‹œì‘
  const startInterviewAfterPermission = async () => {
    try {
      // ë¡œë”© ì‹œì‘
      setIsLoading(true);
      
      // ì§ˆë¬¸ ë°ì´í„° ë¡œë“œ
      await loadQuestions();
      
      setInterviewStage('smalltalk');
      
      const firstQuestion = getRandomSmallTalk();
      setCurrentQuestion(firstQuestion);
      setShowQuestionScript(false);
      
      if (ttsEnabled) {
        await speakTextWithOpenAI(firstQuestion);
        setSpeakButtonActive(true);
      } else {
        setSpeakButtonActive(true);
      }
    } finally {
      // ë¡œë”© ì¢…ë£Œ
      setIsLoading(false);
    }
  };

  // ê¶Œí•œ í™•ì¸
  useEffect(() => {
    const checkPermissions = async () => {
      try {
        console.log('ğŸ¤ Checking permissions...');
        console.log('ğŸ¤ Platform:', Platform.OS);
        
        // ë§ˆì´í¬ ê¶Œí•œ í™•ì¸
        const { status } = await Audio.getPermissionsAsync();
        console.log('ğŸ¤ Microphone permission status:', status);
        
        if (status === 'granted') {
          setMicrophonePermission(true);
          console.log('ğŸ¤ âœ… Microphone permission granted');
        } else {
          setMicrophonePermission(false);
          console.log('ğŸ¤ âŒ Microphone permission not granted');
        }
        
        if (Platform.OS === 'web') {
          // ì›¹ì—ì„œëŠ” Web Speech API í™•ì¸
          const SpeechRecognition = typeof window !== 'undefined' ? (window.SpeechRecognition || window.webkitSpeechRecognition) : null;
          
          if (SpeechRecognition) {
            setRecognitionPermission(true);
            setSttEnabled(true);
            console.log('ğŸ¤ âœ… Web STT enabled');
          } else {
            setRecognitionPermission(false);
            setSttEnabled(false);
            console.log('ğŸ¤ âŒ Web STT not available');
          }
        } else {
          // ëª¨ë°”ì¼ì—ì„œëŠ” OpenAI Whisper + Expo AV ì‚¬ìš© (í•­ìƒ í™œì„±í™”)
          setRecognitionPermission(true);
          setSttEnabled(true);
          console.log('ğŸ¤ âœ… Mobile STT enabled (using OpenAI Whisper)');
        }
      } catch (error) {
        console.error('ğŸ¤ Permission check error:', error);
        setRecognitionPermission(false);
        setSttEnabled(false);
        setMicrophonePermission(false);
      }
    };

    checkPermissions();
  }, []);

  // í™”ë©´ ì§„ì… ì‹œ ì¸í„°ë·° ìë™ ì‹œì‘ (ê¶Œí•œì´ ìˆì„ ë•Œë§Œ)
  useEffect(() => {
    const timer = setTimeout(() => {
      if (microphonePermission) {
        startInterview();
      }
    }, 1000);

    return () => clearTimeout(timer);
  }, [microphonePermission]);

  // í™”ë©´ ì •ë¦¬
  useFocusEffect(
    useCallback(() => {
      console.log('ğŸ”„ AIInterviewScreen focused - initializing...');
      
      // í™”ë©´ ì§„ì… ì‹œ ëª¨ë“  ì´ì „ ì˜¤ë””ì˜¤ ì¤‘ì§€
      const cleanupPreviousAudio = async () => {
        console.log('ğŸ”Š Cleaning up all previous audio on screen entry...');
        
        // 1. Expo Speech ì¤‘ì§€
        try {
          await Speech.stop();
          console.log('ğŸ”Š Stopped Expo Speech');
        } catch (error) {
          console.log('ğŸ”Š Expo Speech stop error:', error);
        }
        
        // 2. ì›¹ ì˜¤ë””ì˜¤ ì¤‘ì§€
        if (typeof window !== 'undefined') {
          if (window.currentAudio) {
            window.currentAudio.pause();
            window.currentAudio.currentTime = 0;
            window.currentAudio = null;
            console.log('ğŸ”Š Stopped web audio');
          }
          
          // 3. Expo AV ì‚¬ìš´ë“œ ì¤‘ì§€
          if (window.currentExpoSound) {
            try {
              await window.currentExpoSound.stopAsync();
              await window.currentExpoSound.unloadAsync();
              window.currentExpoSound = null;
              console.log('ğŸ”Š Stopped Expo AV sound');
            } catch (error) {
              console.log('ğŸ”Š Expo AV stop error:', error);
            }
          }
          
          // 4. ìŒì„± ìƒ˜í”Œ ì¤‘ì§€ (AIChatScreenì—ì„œ ì¬ìƒ ì¤‘ì¼ ìˆ˜ ìˆìŒ)
          if (window.currentSampleSound) {
            try {
              await window.currentSampleSound.stopAsync();
              await window.currentSampleSound.unloadAsync();
              window.currentSampleSound = null;
              console.log('ğŸ”Š Stopped voice sample sound');
            } catch (error) {
              console.log('ğŸ”Š Voice sample stop error:', error);
            }
          }
          
          if (window.currentSampleAudio) {
            window.currentSampleAudio.pause();
            window.currentSampleAudio.currentTime = 0;
            window.currentSampleAudio = null;
            console.log('ğŸ”Š Stopped voice sample audio');
          }
        }
        
        console.log('âœ… Previous audio cleanup completed');
      };
      
      cleanupPreviousAudio();
      
      return async () => {
        console.log('ğŸ”„ AIInterviewScreen unfocused - cleaning up...');
        
        // ëª¨ë“  ëŒ€í™” ìƒíƒœ ì¤‘ë‹¨
        setIsRecording(false);
        setIsSpeaking(false);
        setIsListening(false);
        setIsLoading(false);
        setInterviewStage('waiting');
        setCurrentQuestion('');
        setRealtimeTranscript('');
        setVoiceAnalysisResult(null);
        setSpeakButtonActive(false);
        
        // ì§ˆë¬¸ ìƒíƒœ ì´ˆê¸°í™”
        setCurrentQuestionIndex(0);
        setShuffledQuestions([]);
        setAllQuestions([]);
        setCurrentQuestionOnly('');
        setShowProgressBar(false);
        
        // ì• ë‹ˆë©”ì´ì…˜ ì¤‘ì§€
        
        // TTS ì¤‘ì§€
        try {
          await Speech.stop();
          console.log('ğŸ”„ Stopped Speech');
        } catch (error) {
          console.log('ğŸ”„ Speech stop error:', error);
        }
        
        if (typeof window !== 'undefined') {
          if (window.currentAudio) {
            window.currentAudio.pause();
            window.currentAudio = null;
            console.log('ğŸ”„ Cleaned up web audio');
          }
          if (window.currentExpoSound) {
            try {
              await window.currentExpoSound.unloadAsync();
              window.currentExpoSound = null;
              console.log('ğŸ”„ Cleaned up expo sound');
            } catch (error) {
              console.log('ğŸ”„ Expo sound cleanup error:', error);
            }
          }
        }
        
        // STT ì¤‘ì§€
        if (typeof window !== 'undefined') {
          if (window.currentRecognition) {
            window.currentRecognition.stop();
            window.currentRecognition = null;
            console.log('ğŸ”„ Cleaned up web speech recognition');
          }
          if (window.currentRecording) {
            try {
              await window.currentRecording.stopAndUnloadAsync();
              window.currentRecording = null;
              console.log('ğŸ”„ Cleaned up recording');
            } catch (error) {
              console.log('ğŸ”„ Recording cleanup error:', error);
            }
          }
        }
        
        console.log('âœ… AIInterviewScreen cleanup completed');
      };
    }, [])
  );

  return (
    <SafeAreaView style={styles.container}>
      <StatusBar barStyle="dark-content" backgroundColor="#fff" />
      
      {/* í—¤ë” */}
      <View style={styles.header}>
        <TouchableOpacity
          style={styles.backButton}
          onPress={() => navigation.goBack()}
        >
          <Ionicons name="arrow-back" size={24} color="#007AFF" />
        </TouchableOpacity>
        
        <Text style={styles.headerTitle}>AI Interview</Text>
        
        <View style={styles.headerRight}>
          <TouchableOpacity
            style={styles.headerButton}
            onPress={() => setTtsEnabled(!ttsEnabled)}
          >
            <Ionicons 
              name={ttsEnabled ? "volume-high" : "volume-mute"} 
              size={20} 
              color={ttsEnabled ? "#007AFF" : "#999"} 
            />
          </TouchableOpacity>
        </View>
      </View>

      {/* ë§ˆì´í¬ ê¶Œí•œ ì•Œë¦¼ */}
      {!microphonePermission && (
        <View style={styles.permissionWarning}>
          <Ionicons name="mic-off" size={20} color="#FF3B30" />
          <Text style={styles.permissionWarningText}>
            ğŸ¤ Microphone permission is required for AI Interview
          </Text>
        </View>
      )}

      {/* ì¸í„°ë·° ì˜ì—­ */}
      <View style={styles.interviewContainer}>
        {/* ì§„í–‰ë„ í‘œì‹œ ë°” */}
        {showProgressBar && (
          <View style={styles.progressContainer}>
            <Text style={styles.progressText}>
              Progress: {currentQuestionIndex}/{shuffledQuestions.length}
            </Text>
            <View style={styles.progressBarBackground}>
              <View 
                style={[
                  styles.progressBarFill,
                  { width: `${(currentQuestionIndex / shuffledQuestions.length) * 100}%` }
                ]}
              />
            </View>
            <Text style={styles.progressPercentage}>
              {Math.round((currentQuestionIndex / shuffledQuestions.length) * 100)}%
            </Text>
          </View>
        )}
        
        {/* ì§ˆë¬¸ ì¹´ë“œ */}
        <TouchableOpacity 
          style={styles.questionCard}
          onPress={() => setShowQuestionScript(!showQuestionScript)}
        >
          <View style={styles.questionCardHeader}>
            <Text style={styles.questionCardTitle}>
              {t('menu.aiInterview.citizenshipQuestion')}
            </Text>
            <Ionicons 
              name={showQuestionScript ? "eye-off" : "eye"} 
              size={20} 
              color="#666" 
            />
          </View>
          <Text style={styles.questionText}>
            {showQuestionScript 
              ? (interviewStage === 'interview' && currentQuestionOnly 
                  ? currentQuestionOnly 
                  : currentQuestion)
              : (interviewStage === 'smalltalk' 
                  ? t('menu.aiInterview.touchToViewInterviewerChat')
                  : t('menu.aiInterview.touchToViewCitizenshipQuestion'))
            }
          </Text>
          {!showQuestionScript && (
            <Text style={styles.questionHint}>
              ğŸ’¡ {t('menu.aiInterview.listenAndAnswer')}
            </Text>
          )}
        </TouchableOpacity>

        {/* ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸ ë°•ìŠ¤ */}
        <View style={styles.transcriptContainer}>
          <View style={styles.transcriptHeader}>
            <View style={styles.answerHeaderLeft}>
              <Ionicons name="chatbubble-outline" size={20} color="#333" style={styles.waveIcon} />
              <Text style={styles.transcriptTitle}>Your Answer: </Text>
            </View>
            {interviewStage === 'interview' && currentQuestionOnly && (
              <TouchableOpacity 
                style={styles.answerButton}
                onPress={() => {
                  console.log('â“ Answer button pressed');
                  setShowAnswer(true);
                }}
              >
                <Text style={styles.answerButtonText}>?</Text>
              </TouchableOpacity>
            )}
          </View>
          {isListening && (
            <View style={styles.listeningIndicator}>
              <Text style={styles.listeningText}>ë“£ëŠ” ì¤‘...</Text>
              <View style={styles.listeningDot} />
            </View>
          )}
          
          <View style={styles.transcriptContent}>
            {voiceAnalysisResult && (
              <View style={styles.voiceAnalysisBox}>
                <Text style={styles.voiceAnalysisTitle}>ìŒì„± ë¶„ì„ ê²°ê³¼:</Text>
                <Text style={styles.voiceAnalysisText}>
                  {voiceAnalysisResult.analysis}
                </Text>
                <View style={styles.voiceDetails}>
                  <Text style={styles.voiceDetailItem}>
                    í’ˆì§ˆ: {voiceAnalysisResult.quality}
                  </Text>
                  <Text style={styles.voiceDetailItem}>
                    í†¤: {voiceAnalysisResult.tone}
                  </Text>
                </View>
              </View>
            )}
            
            <Text style={styles.transcriptText}>
              {realtimeTranscript || t('menu.aiInterview.pressSpeakButtonToStart')}
            </Text>
          </View>
        </View>

        {/* ë§í•˜ê¸° ë²„íŠ¼ */}
        <View style={styles.speakButtonContainer}>
          <TouchableOpacity
              style={[
                styles.speakButton,
                speakButtonActive && styles.speakButtonActive,
                isRecording && styles.speakButtonRecording
              ]}
              onPress={isRecording ? stopSpeechRecognition : startSpeechRecognition}
              disabled={!speakButtonActive || isLoading}
            >
            <Ionicons 
              name={isRecording ? "stop-circle" : "mic-circle"} 
              size={80} 
              color={isRecording ? "#FF3B30" : (speakButtonActive ? "#007AFF" : "#CCC")} 
            />
            
            {/* ë²„íŠ¼ í…ìŠ¤íŠ¸ */}
            <Text style={[
              styles.speakButtonText,
              speakButtonActive && styles.speakButtonTextActive
            ]}>
              {isRecording ? t('menu.aiInterview.stopAnswer') : t('menu.aiInterview.answer')}
            </Text>
          </TouchableOpacity>
        </View>

        {/* ë¡œë”© ìƒíƒœ */}
        {isLoading && (
          <View style={styles.interviewLoadingContainer}>
            <ActivityIndicator size="large" color="#007AFF" />
            <Text style={styles.interviewLoadingText}>ğŸ¤– AIê°€ ì‘ë‹µì„ ì¤€ë¹„í•˜ëŠ” ì¤‘...</Text>
          </View>
        )}
      </View>

      {/* ì •ë‹µ í‘œì‹œ Modal */}
      <Modal
        visible={showAnswer}
        transparent={true}
        animationType="fade"
        onRequestClose={() => setShowAnswer(false)}
      >
        <View style={styles.modalOverlay}>
          <View style={styles.modalContent}>
            <View style={styles.modalHeader}>
              <Text style={styles.modalTitle}>Question & Answer</Text>
              <TouchableOpacity
                style={styles.closeButton}
                onPress={() => setShowAnswer(false)}
              >
                <Ionicons name="close" size={24} color="#666" />
              </TouchableOpacity>
            </View>
            
            <ScrollView style={styles.modalBody}>
              {/* ì§ˆë¬¸ í‘œì‹œ */}
              {interviewStage === 'interview' && currentQuestionIndex > 0 && 
               shuffledQuestions[currentQuestionIndex - 1] && (
                <>
                  <View style={styles.modalQuestion}>
                    <Text style={styles.modalQuestionText}>
                      {shuffledQuestions[currentQuestionIndex - 1].question}
                    </Text>
                  </View>
                  
                  {/* ì •ë‹µ í‘œì‹œ */}
                  {shuffledQuestions[currentQuestionIndex - 1].correctAnswers && (
                    <View style={styles.modalAnswer}>
                      <Text style={styles.modalSectionTitle}>Correct Answer:</Text>
                      {shuffledQuestions[currentQuestionIndex - 1].correctAnswers.map((answerObj, idx) => (
                        <View key={idx} style={styles.answerItem}>
                          <Text style={styles.modalCorrectAnswer}>â€¢ {answerObj.text}</Text>
                          {idx < shuffledQuestions[currentQuestionIndex - 1].correctAnswers.length - 1 && (
                            <View style={styles.answerSeparator} />
                          )}
                        </View>
                      ))}
                    </View>
                  )}
                </>
              )}
            </ScrollView>
          </View>
        </View>
      </Modal>
    </SafeAreaView>
  );
};

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#f8f9fa',
  },
  header: {
    flexDirection: 'row',
    alignItems: 'center',
    justifyContent: 'space-between',
    paddingHorizontal: 16,
    paddingVertical: 12,
    backgroundColor: '#fff',
    borderBottomWidth: 1,
    borderBottomColor: '#e9ecef',
  },
  backButton: {
    padding: 8,
  },
  headerTitle: {
    fontSize: 18,
    fontWeight: 'bold',
    color: '#333',
  },
  headerRight: {
    flexDirection: 'row',
    alignItems: 'center',
  },
  headerButton: {
    padding: 8,
    marginLeft: 8,
  },
  permissionWarning: {
    flexDirection: 'row',
    alignItems: 'center',
    justifyContent: 'center',
    backgroundColor: '#FFF3CD',
    paddingVertical: 12,
    paddingHorizontal: 16,
    borderBottomWidth: 1,
    borderBottomColor: '#FFE69C',
    gap: 8,
  },
  permissionWarningText: {
    fontSize: 14,
    color: '#856404',
    fontWeight: '600',
  },
  interviewContainer: {
    flex: 1,
    paddingHorizontal: 20,
    paddingVertical: 20,
  },
  questionCard: {
    backgroundColor: '#fff',
    borderRadius: 15,
    padding: 20,
    marginBottom: 30,
    shadowColor: '#000',
    shadowOffset: {
      width: 0,
      height: 2,
    },
    shadowOpacity: 0.1,
    shadowRadius: 8,
    elevation: 5,
    borderWidth: 1,
    borderColor: '#e9ecef',
  },
  questionCardHeader: {
    flexDirection: 'row',
    justifyContent: 'space-between',
    alignItems: 'center',
    marginBottom: 15,
  },
  questionCardTitle: {
    fontSize: 16,
    fontWeight: 'bold',
    color: '#333',
  },
  questionText: {
    fontSize: 18,
    lineHeight: 26,
    color: '#333',
    textAlign: 'center',
    minHeight: 60,
  },
  questionHint: {
    fontSize: 14,
    color: '#666',
    textAlign: 'center',
    marginTop: 10,
    fontStyle: 'italic',
  },
  speakButtonContainer: {
    flex: 1,
    justifyContent: 'center',
    alignItems: 'center',
  },
  speakButton: {
    alignItems: 'center',
    justifyContent: 'center',
    padding: 20,
  },
  speakButtonActive: {
    transform: [{ scale: 1.1 }],
  },
  speakButtonRecording: {
    transform: [{ scale: 1.2 }],
  },
  speakButtonText: {
    fontSize: 18,
    color: '#CCC',
    marginTop: 15,
    fontWeight: '600',
  },
  speakButtonTextActive: {
    color: '#007AFF',
  },
  interviewLoadingContainer: {
    alignItems: 'center',
    justifyContent: 'center',
    paddingVertical: 30,
    backgroundColor: '#f0f8ff',
    borderRadius: 12,
    marginHorizontal: 20,
    marginVertical: 10,
    borderWidth: 1,
    borderColor: '#007AFF',
  },
  interviewLoadingText: {
    fontSize: 16,
    color: '#007AFF',
    marginTop: 15,
    textAlign: 'center',
    fontWeight: '600',
  },
  // ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸ ë°•ìŠ¤ ìŠ¤íƒ€ì¼
  transcriptContainer: {
    backgroundColor: '#f8f9fa',
    borderRadius: 15,
    padding: 15,
    marginBottom: 20,
    borderWidth: 1,
    borderColor: '#e9ecef',
  },
  transcriptHeader: {
    flexDirection: 'row',
    justifyContent: 'space-between',
    alignItems: 'center',
    marginBottom: 12,
  },
  answerHeaderLeft: {
    flexDirection: 'row',
    alignItems: 'center',
  },
  waveIcon: {
    marginRight: 8,
  },
  transcriptTitle: {
    fontSize: 16,
    fontWeight: 'bold',
    color: '#333',
  },
  answerButton: {
    width: 32,
    height: 32,
    borderRadius: 16,
    backgroundColor: '#007AFF',
    justifyContent: 'center',
    alignItems: 'center',
    shadowColor: '#000',
    shadowOffset: { width: 0, height: 2 },
    shadowOpacity: 0.2,
    shadowRadius: 3,
    elevation: 3,
  },
  answerButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: 'bold',
  },
  answerBox: {
    backgroundColor: '#e8f5e9',
    borderRadius: 10,
    padding: 15,
    marginBottom: 15,
    borderWidth: 1,
    borderColor: '#4caf50',
  },
  answerBoxTitle: {
    fontSize: 14,
    fontWeight: 'bold',
    color: '#2e7d32',
    marginBottom: 8,
  },
  answerBoxText: {
    fontSize: 15,
    color: '#1b5e20',
    lineHeight: 22,
  },
  listeningIndicator: {
    flexDirection: 'row',
    alignItems: 'center',
  },
  listeningText: {
    fontSize: 12,
    color: '#007AFF',
    marginRight: 6,
  },
  listeningDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
    backgroundColor: '#007AFF',
    opacity: 0.8,
  },
  transcriptContent: {
  },
  // ì§„í–‰ë„ ë°” ìŠ¤íƒ€ì¼
  progressContainer: {
    backgroundColor: '#f8f9fa',
    borderRadius: 12,
    padding: 15,
    marginBottom: 15,
    borderWidth: 1,
    borderColor: '#e9ecef',
  },
  progressText: {
    fontSize: 14,
    fontWeight: '600',
    color: '#333',
    marginBottom: 8,
    textAlign: 'center',
  },
  progressBarBackground: {
    height: 8,
    backgroundColor: '#e9ecef',
    borderRadius: 4,
    marginBottom: 8,
    overflow: 'hidden',
  },
  progressBarFill: {
    height: '100%',
    backgroundColor: '#007AFF',
    borderRadius: 4,
    minWidth: 2,
  },
  progressPercentage: {
    fontSize: 12,
    color: '#666',
    textAlign: 'center',
    fontWeight: '500',
  },
  voiceAnalysisBox: {
    backgroundColor: '#e3f2fd',
    borderRadius: 10,
    padding: 12,
    marginBottom: 10,
    borderLeftWidth: 4,
    borderLeftColor: '#2196f3',
  },
  voiceAnalysisTitle: {
    fontSize: 14,
    fontWeight: 'bold',
    color: '#1976d2',
    marginBottom: 6,
  },
  voiceAnalysisText: {
    fontSize: 14,
    color: '#333',
    marginBottom: 8,
  },
  voiceDetails: {
    flexDirection: 'row',
    justifyContent: 'space-between',
  },
  voiceDetailItem: {
    fontSize: 12,
    color: '#666',
    backgroundColor: '#fff',
    paddingHorizontal: 8,
    paddingVertical: 4,
    borderRadius: 6,
    overflow: 'hidden',
  },
  transcriptText: {
    fontSize: 16,
    lineHeight: 22,
    color: '#333',
    backgroundColor: '#fff',
    padding: 12,
    borderRadius: 8,
    minHeight: 80,
    maxHeight: 200,
    textAlignVertical: 'top',
    flexWrap: 'wrap',
  },
  // Modal ìŠ¤íƒ€ì¼ (Story ëª¨ë“œì™€ ë™ì¼)
  modalOverlay: {
    flex: 1,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    justifyContent: 'center',
    alignItems: 'center',
  },
  modalContent: {
    backgroundColor: '#fff',
    borderRadius: 16,
    width: '90%',
    maxHeight: '80%',
    overflow: 'hidden',
  },
  modalHeader: {
    flexDirection: 'row',
    alignItems: 'center',
    justifyContent: 'space-between',
    padding: 20,
    borderBottomWidth: 1,
    borderBottomColor: '#e9ecef',
  },
  modalTitle: {
    fontSize: 18,
    fontWeight: 'bold',
    color: '#333',
  },
  closeButton: {
    padding: 4,
  },
  modalBody: {
    padding: 20,
  },
  modalQuestion: {
    marginBottom: 20,
  },
  modalQuestionText: {
    fontSize: 18,
    fontWeight: '600',
    color: '#333',
    lineHeight: 26,
  },
  modalAnswer: {
    marginBottom: 20,
  },
  modalSectionTitle: {
    fontSize: 16,
    fontWeight: 'bold',
    color: '#2E86AB',
    marginBottom: 8,
  },
  modalCorrectAnswer: {
    fontSize: 16,
    color: '#28a745',
    fontWeight: '600',
    lineHeight: 24,
  },
  answerItem: {
    marginBottom: 12,
  },
  answerSeparator: {
    height: 1,
    backgroundColor: '#e0e0e0',
    marginTop: 8,
    marginBottom: 4,
  },
});

export default AIInterviewScreen;
